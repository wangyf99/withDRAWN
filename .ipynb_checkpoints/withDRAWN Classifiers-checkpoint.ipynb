{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbea6394-7690-4230-856f-717d378475a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f00ac86-aad9-4aa1-8635-7956dcd7a80c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: TPOT 1.1.0\n",
      "Uninstalling TPOT-1.1.0:\n",
      "  Successfully uninstalled TPOT-1.1.0\n",
      "Collecting git+https://github.com/EpistasisLab/tpot.git\n",
      "  Cloning https://github.com/EpistasisLab/tpot.git to /private/var/folders/16/gzssnc2x4qz5n4rkpvt7tsjh0000gn/T/pip-req-build-yzu8qp9b\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/EpistasisLab/tpot.git /private/var/folders/16/gzssnc2x4qz5n4rkpvt7tsjh0000gn/T/pip-req-build-yzu8qp9b\n",
      "  Resolved https://github.com/EpistasisLab/tpot.git to commit 1bca6c6a51a79dc7370fbd2a8864a561fc5d7529\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.26.4 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (1.11.4)\n",
      "Collecting scikit-learn>=1.6 (from TPOT==1.1.1.dev9+g1bca6c6a5)\n",
      "  Using cached scikit_learn-1.7.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: update_checker>=0.16 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (0.18.0)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (4.67.1)\n",
      "Requirement already satisfied: stopit>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (1.1.2)\n",
      "Requirement already satisfied: pandas>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (2.3.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (1.2.0)\n",
      "Requirement already satisfied: xgboost>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (3.0.5)\n",
      "Requirement already satisfied: matplotlib>=3.6.2 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (3.8.0)\n",
      "Requirement already satisfied: traitlets>=5.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (5.14.3)\n",
      "Requirement already satisfied: lightgbm>=3.3.3 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (4.6.0)\n",
      "Requirement already satisfied: optuna>=3.0.5 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (4.5.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (3.5)\n",
      "Requirement already satisfied: dask>=2024.4.2 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (2025.9.1)\n",
      "Requirement already satisfied: distributed>=2024.4.2 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (2025.9.1)\n",
      "Requirement already satisfied: dask-expr>=1.0.12 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (2.0.0)\n",
      "Requirement already satisfied: dask-jobqueue>=0.8.5 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (0.9.0)\n",
      "Requirement already satisfied: func_timeout>=4.3.5 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (4.3.5)\n",
      "Requirement already satisfied: configspace>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (1.2.1)\n",
      "Requirement already satisfied: dill>=0.3.9 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (0.4.0)\n",
      "Requirement already satisfied: seaborn>=0.13.2 in /opt/anaconda3/lib/python3.11/site-packages (from TPOT==1.1.1.dev9+g1bca6c6a5) (0.13.2)\n",
      "Requirement already satisfied: pyparsing in /opt/anaconda3/lib/python3.11/site-packages (from configspace>=1.1.1->TPOT==1.1.1.dev9+g1bca6c6a5) (3.0.9)\n",
      "Requirement already satisfied: typing_extensions in /opt/anaconda3/lib/python3.11/site-packages (from configspace>=1.1.1->TPOT==1.1.1.dev9+g1bca6c6a5) (4.15.0)\n",
      "Requirement already satisfied: more_itertools in /opt/anaconda3/lib/python3.11/site-packages (from configspace>=1.1.1->TPOT==1.1.1.dev9+g1bca6c6a5) (10.1.0)\n",
      "Requirement already satisfied: click>=8.1 in /opt/anaconda3/lib/python3.11/site-packages (from dask>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from dask>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (3.1.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /opt/anaconda3/lib/python3.11/site-packages (from dask>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from dask>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (23.1)\n",
      "Requirement already satisfied: partd>=1.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from dask>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (1.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from dask>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from dask>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (0.12.0)\n",
      "Requirement already satisfied: importlib_metadata>=4.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from dask>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (7.0.1)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (3.1.3)\n",
      "Requirement already satisfied: locket>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (1.0.0)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (1.0.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (5.9.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (1.7.0)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (6.3.3)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (2.0.7)\n",
      "Requirement already satisfied: zict>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (3.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.6.2->TPOT==1.1.1.dev9+g1bca6c6a5) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.6.2->TPOT==1.1.1.dev9+g1bca6c6a5) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.6.2->TPOT==1.1.1.dev9+g1bca6c6a5) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.6.2->TPOT==1.1.1.dev9+g1bca6c6a5) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.6.2->TPOT==1.1.1.dev9+g1bca6c6a5) (10.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.6.2->TPOT==1.1.1.dev9+g1bca6c6a5) (2.8.2)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from optuna>=3.0.5->TPOT==1.1.1.dev9+g1bca6c6a5) (1.16.5)\n",
      "Requirement already satisfied: colorlog in /opt/anaconda3/lib/python3.11/site-packages (from optuna>=3.0.5->TPOT==1.1.1.dev9+g1bca6c6a5) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/anaconda3/lib/python3.11/site-packages (from optuna>=3.0.5->TPOT==1.1.1.dev9+g1bca6c6a5) (2.0.25)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=2.2.0->TPOT==1.1.1.dev9+g1bca6c6a5) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=2.2.0->TPOT==1.1.1.dev9+g1bca6c6a5) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.6->TPOT==1.1.1.dev9+g1bca6c6a5) (3.6.0)\n",
      "Requirement already satisfied: requests>=2.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from update_checker>=0.16->TPOT==1.1.1.dev9+g1bca6c6a5) (2.32.5)\n",
      "Requirement already satisfied: Mako in /opt/anaconda3/lib/python3.11/site-packages (from alembic>=1.5.0->optuna>=3.0.5->TPOT==1.1.1.dev9+g1bca6c6a5) (1.3.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.11/site-packages (from importlib_metadata>=4.13.0->dask>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2>=2.10.3->distributed>=2024.4.2->TPOT==1.1.1.dev9+g1bca6c6a5) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.6.2->TPOT==1.1.1.dev9+g1bca6c6a5) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.3.0->update_checker>=0.16->TPOT==1.1.1.dev9+g1bca6c6a5) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.3.0->update_checker>=0.16->TPOT==1.1.1.dev9+g1bca6c6a5) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.3.0->update_checker>=0.16->TPOT==1.1.1.dev9+g1bca6c6a5) (2024.8.30)\n",
      "Using cached scikit_learn-1.7.2-cp311-cp311-macosx_12_0_arm64.whl (8.6 MB)\n",
      "Building wheels for collected packages: TPOT\n",
      "  Building wheel for TPOT (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for TPOT: filename=tpot-1.1.1.dev9+g1bca6c6a5-py3-none-any.whl size=224702 sha256=3e9e2fb8ad738f8621419cc086138c13e1cd941b0a681a0c95ce22256921c821\n",
      "  Stored in directory: /private/var/folders/16/gzssnc2x4qz5n4rkpvt7tsjh0000gn/T/pip-ephem-wheel-cache-7fia6ksl/wheels/6d/42/bb/ac104d580c2c70800b482a5e8da4f6320c3f0bd31f9569510d\n",
      "Successfully built TPOT\n",
      "Installing collected packages: scikit-learn, TPOT\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.3.2\n",
      "    Uninstalling scikit-learn-1.3.2:\n",
      "      Successfully uninstalled scikit-learn-1.3.2\n",
      "Successfully installed TPOT-1.1.1.dev9+g1bca6c6a5 scikit-learn-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y tpot\n",
    "!pip install git+https://github.com/EpistasisLab/tpot.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e02cc621-05b2-4c99-9e37-93ba13f89111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.1.dev9+g1bca6c6a5\n"
     ]
    }
   ],
   "source": [
    "import tpot\n",
    "print(tpot.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55363aea-9c7d-447f-8fcc-a1256ba775f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tox_pred.py\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, chi2\n",
    "\n",
    "def load_labels(label_key_number, random_seed, train_percent):\n",
    "    '''loads drugs corresponding to a ballanced training and two test sets\n",
    "\n",
    "    Input:\n",
    "    label_key_number (int)- column in the label file to use as the labels\n",
    "    random_seed (int) - controls the randomness for replicability if desired\n",
    "    train_percent (float) - percent of the set used for testing\n",
    "\n",
    "    Output: (train, test1, test2, Ltrain, Ltest1, Ltest2) (tuple of lists) lists of labels and lists of drug names corresponding to a ballanced training and two test sets\n",
    "    '''\n",
    "    positive = []\n",
    "    negative = []\n",
    "    with open('tox_labels.csv') as fo:\n",
    "        i = 0 \n",
    "        for line in fo:\n",
    "            if i != 0: \n",
    "                split_line = line[:-1].split(',')\n",
    "                if split_line[label_key_number] == '1':\n",
    "                    positive.append(split_line[0].lower())\n",
    "                else:\n",
    "                    negative.append(split_line[0].lower())\n",
    "            i += 1\n",
    "\n",
    "    random.seed(random_seed)\n",
    "    size_samples = math.floor(min(len(positive)*(train_percent), len(negative)*(train_percent)))\n",
    "    size_test = math.floor(min((len(positive)*(1-train_percent))/2, (len(negative)*(1-train_percent))/2))\n",
    "\n",
    "    rearrange_positive = random.sample(positive, len(positive))\n",
    "    rearrange_negative = random.sample(negative, len(negative))\n",
    "\n",
    "    train = rearrange_positive[:size_samples] + rearrange_negative[:size_samples] \n",
    "    test1 = rearrange_positive[size_samples:size_samples+size_test] + rearrange_negative[size_samples:size_samples+size_test] \n",
    "    test2 = rearrange_positive[size_samples+size_test:size_samples+2*size_test] + rearrange_negative[size_samples+size_test:size_samples+2*size_test]\n",
    "    Ltrain = [1]*len(rearrange_positive[:size_samples]) + [0]*len(rearrange_negative[:size_samples])\n",
    "    Ltest1 = [1]*len(rearrange_positive[size_samples:size_samples+size_test]) + [0]*len(rearrange_negative[size_samples:size_samples+size_test])\n",
    "    Ltest2 = [1]*len(rearrange_positive[size_samples+size_test:size_samples+2*size_test]) + [0]*len(rearrange_negative[size_samples+size_test:size_samples+2*size_test])\n",
    "\n",
    "    return train, test1, test2, Ltrain, Ltest1, Ltest2\n",
    "\n",
    "def load_nongraph(drug_names_list, filename):\n",
    "    ''' load data for the drugs in the list from the file\n",
    "    \n",
    "    Inputs:\n",
    "    drug_names_list (list of str)\n",
    "    filename (str)\n",
    "\n",
    "    Output: (list of lists) drug data matrix\n",
    "    '''\n",
    "    track = {}\n",
    "    with open(filename) as fo:\n",
    "        for line in fo:\n",
    "            split_line = line[:-1].split(',')\n",
    "            convert_to_float = []\n",
    "            for elt in split_line[1:]:\n",
    "                try:\n",
    "                    convert_to_float.append(float(elt))\n",
    "                except:\n",
    "                    convert_to_float.append(0)\n",
    "            track[split_line[0].lower()] = convert_to_float\n",
    "    out = []\n",
    "    for drug in drug_names_list:\n",
    "        out.append(track[drug])\n",
    "    return out\n",
    "\n",
    "def load_data(drug_names_list):\n",
    "    ''' load drug data for all predictors\n",
    "\n",
    "    Input:\n",
    "    drug_names_list (list of str)\n",
    "\n",
    "    Output: (sages_out, fp_out, drug_features_out, targetsall) (tuple of pandas data frames) datasets for each of the predictors\n",
    "    '''\n",
    "    sages_out = pd.DataFrame(load_nongraph(drug_names_list, 'sages.csv'))\n",
    "    fp_out = pd.DataFrame(load_nongraph(drug_names_list, 'fp.csv'))\n",
    "    drug_features_out = pd.DataFrame(load_nongraph(drug_names_list, 'drug_features.csv'))\n",
    "    targetsall = pd.DataFrame(load_nongraph(drug_names_list, 'targetsall.csv'))\n",
    "    return sages_out, fp_out, drug_features_out, targetsall\n",
    "\n",
    "def norm_data_by_train(trainset, testset1):\n",
    "    '''minmax normalizes the data by the traiining set\n",
    "\n",
    "    Inputs:\n",
    "    trainset (pandas data frame)\n",
    "    testset1 (pandas data frame)\n",
    "\n",
    "    Output: normalized pandas data frame of the testset \n",
    "    '''\n",
    "    mins = trainset.min(axis=0)\n",
    "    maxs = trainset.max(axis=0) \n",
    "    for colnumber in range(testset1.shape[1]):\n",
    "        mi = mins[colnumber]\n",
    "        ma = maxs[colnumber]\n",
    "        testset1[colnumber] = (testset1[colnumber]-mi)/(ma-mi)\n",
    "    return testset1.fillna(0)\n",
    "\n",
    "def evaluate(y_test, y_test_predict):\n",
    "    '''returns performance metrics for machine learning classifiers\n",
    "\n",
    "    Inputs:\n",
    "    y_test (list) true values of the labels\n",
    "    y_test_predict (list) predicted values of the labels output from the classifier\n",
    "\n",
    "    Output: acc,aroc,f1_val,precision_val,recall_val,mcc (tupl of floats) performance metric values\n",
    "    '''\n",
    "    acc = accuracy_score(y_test, y_test_predict)\n",
    "    aroc = roc_auc_score(y_test, y_test_predict)\n",
    "    f1_val = f1_score(y_test, y_test_predict)\n",
    "    precision_val = precision_score(y_test, y_test_predict)\n",
    "    recall_val = recall_score(y_test, y_test_predict)\n",
    "    mcc = matthews_corrcoef(y_test, y_test_predict)\n",
    "    return acc,aroc,f1_val,precision_val,recall_val,mcc\n",
    "\n",
    "def split_norm_data(train, test1, test2):\n",
    "    ''' loads the data and normalizes by the training set\n",
    "\n",
    "    Inputs: train, test1, test2 each (list) contains names of the drugs (str) in the data subset \n",
    "\n",
    "    Output:\n",
    "    list of tuples where the first index of the tuple is the dataset label (str) and the remainder are pandas dataframes corresponding to train_set,test_set1,test_set2\n",
    "    '''\n",
    "    load_train = load_data(train)\n",
    "    load_test1 = load_data(test1) \n",
    "    load_test2 = load_data(test2)\n",
    "    l = ['sages', 'fp', 'drug_features', 'targetsall']\n",
    "    out = []\n",
    "    for data_i in range(len(load_train)):\n",
    "        train_set = norm_data_by_train(load_train[data_i],load_train[data_i])\n",
    "        test_set1 = norm_data_by_train(load_train[data_i],load_test1[data_i])\n",
    "        test_set2 = norm_data_by_train(load_train[data_i],load_test2[data_i])\n",
    "        out.append((l[data_i], train_set,test_set1,test_set2))\n",
    "    return out\n",
    "\n",
    "\n",
    "def tuning_level1(label_key_number, random_seed, train_percent,classifiers, cl, outdir, write=False):\n",
    "    '''model selection and hyperparameter tuning for each of the datasets\n",
    "\n",
    "    Inputs:\n",
    "    label_key_number (int) column number corresponding to which labels to use in the tox_labels.csv file\n",
    "    random_seed (int) for model instantiation and dataset splitting\n",
    "    train_percent (float) amount of the dataset used for training, the remaning data will be split into two test sets\n",
    "    classifiers (list of TPOT classifiers)\n",
    "    cl (list of str) classifier labels with the same indexing as classifiers\n",
    "    outdir (str) directory where output files will be saved\n",
    "    write (boolean) for debugging purposes, False will prevent any files from being saved\n",
    "    \n",
    "    Outputs: returns None, but will save the following files (if the write variable is True)\n",
    "    classifier name - dataset -level1-tpot_exported_pipeline.py: a python file with the best hyperparameter tuned classifer\n",
    "    dataset -level1_out_train_labels.csv: labels for the training data set for the ensemble\n",
    "    dataset -level1_out_test_labels.csv: labels for the test set for the ensemble\n",
    "    level1_summary.csv: performance metrics for all classifiers trained on all datasets\n",
    "    dataset - classifier - random seed -level2_train.csv: predictions for the classifier on the data set, used as training data for the ensemble\n",
    "    dataset - classifier - random seed -level2_test.csv: predictions for the classifier on the data set, used as testing data for the ensemble\n",
    "    '''\n",
    "    train, test1, test2, Ltrain, Ltest1, Ltest2 = load_labels(label_key_number, random_seed, train_percent)\n",
    "    data = split_norm_data(train, test1, test2)\n",
    "    if write:\n",
    "        fout0 = open(outdir+'level1_summary.csv', '+a')\n",
    "        fout0.write('RandomSeed,Data,Accuracy,AUROC,F1,Precision,Recall,MCC,Classifier\\n')\n",
    "        fout0.close()\n",
    "    \n",
    "    for data_set_i in range(len(data)):\n",
    "        print('****************')\n",
    "        data_set = data[data_set_i]\n",
    "        print(data_set[0])\n",
    "        train_set = data_set[1]\n",
    "        test_set1 = data_set[2]\n",
    "        test_set2 = data_set[3]\n",
    "        \n",
    "        for clf_i in range(len(classifiers)):\n",
    "            clf = classifiers[clf_i]\n",
    "            clf.fit(train_set,np.array(Ltrain))\n",
    "            exctracted_best_model = clf.fitted_pipeline_.steps[-1][1]\n",
    "            if write:\n",
    "                clf.export(outdir+cl[clf_i]+ '-' + data_set[0]+'-level1-tpot_exported_pipeline.py')\n",
    "            for rs in range(10):\n",
    "                rstrain, rstest1, rstest2, rsLtrain, rsLtest1, rsLtest2 = load_labels(label_key_number, rs, train_percent)\n",
    "                rsdata = split_norm_data(rstrain, rstest1, rstest2)\n",
    "                rstrain_set = rsdata[data_set_i][1]\n",
    "                rstest_set1 = rsdata[data_set_i][2]\n",
    "                rstest_set2 = rsdata[data_set_i][3]\n",
    "                rsmodel = exctracted_best_model.fit(rstrain_set,np.array(rsLtrain))\n",
    "\n",
    "                y_predict_test1 = rsmodel.predict(rstest_set1)\n",
    "                \n",
    "                acc,aroc,f1_val,precision_val,recall_val,mcc = evaluate(np.array(rsLtest1), y_predict_test1)\n",
    "                \n",
    "                if write:\n",
    "                    fout1 = open( outdir + data_set[0]+'-level1_out_train_labels.csv','a+')\n",
    "                    for elt in rsLtrain+rsLtest1:\n",
    "                        fout1.write(str(elt)+ ',')\n",
    "                    fout1.write('\\n')\n",
    "                    fout1.close()\n",
    "                    fout2 = open(outdir + data_set[0]+'-level1_out_test_labels.csv' ,'a+')\n",
    "                    for elt in rsLtest2:\n",
    "                        fout2.write(str(elt)+ ',')\n",
    "                    fout2.write('\\n')\n",
    "                    fout2.close()\n",
    "                    out_str = str(rs)+','+data_set[0]+','+str(acc)+','+str(aroc)+','+str(f1_val)+','+str(precision_val)+','+str(recall_val) +','+str(mcc)+','+ cl[clf_i] +'\\n'\n",
    "                    fout0 = open(outdir+'level1_summary.csv', '+a')\n",
    "                    fout0.write(out_str)\n",
    "                    fout0.close()\n",
    "\n",
    "                    try:\n",
    "                        y_predict_test1 = rsmodel.predict_proba(rstest_set1)\n",
    "                        y_predict_test2 = rsmodel.predict_proba(rstest_set2)\n",
    "                        y_predict_train = rsmodel.predict_proba(rstrain_set)\n",
    "                        fout3 = open( outdir + data_set[0]+'-'+cl[clf_i]+'-'+str(rs)+'-level2_train.csv','a+')\n",
    "                        for elt in list(y_predict_train) + list(y_predict_test1):\n",
    "                            fout3.write(str(elt[0])+ ',')\n",
    "                        fout3.write('\\n')\n",
    "                        fout3.close()\n",
    "                        fout4 = open( outdir+ data_set[0]+'-'+ cl[clf_i]+'-'+str(rs)+'-level2_test.csv','a+')\n",
    "                        for elt in list(y_predict_test2):\n",
    "                            fout4.write(str(elt[0])+ ',')\n",
    "                        fout4.write('\\n')\n",
    "                        fout4.close()\n",
    "                    except:\n",
    "                        y_predict_test2 = rsmodel.predict(rstest_set2)\n",
    "                        y_predict_train = rsmodel.predict(rstrain_set)\n",
    "                        fout3 = open( outdir + data_set[0]+'-'+cl[clf_i]+'-'+str(rs)+'-level2_train.csv','a+')\n",
    "                        for elt in list(y_predict_train) + list(y_predict_test1):\n",
    "                            fout3.write(str(elt)+ ',')\n",
    "                        fout3.write('\\n')\n",
    "                        fout3.close()\n",
    "                        fout4 = open( outdir+ data_set[0]+'-'+ cl[clf_i]+'-'+str(rs)+'-level2_test.csv','a+')\n",
    "                        for elt in list(y_predict_test2):\n",
    "                            fout4.write(str(elt)+ ',')\n",
    "                        fout4.write('\\n')\n",
    "                        fout4.close()\n",
    "\n",
    "def get_label_from_l1(outdir, train_or_test):\n",
    "    '''reads the file containing labels of the training and test sets for the ensemble\n",
    "\n",
    "    Inputs:\n",
    "    outdir (str) directory where the file containing labels of the training and test sets can be found\n",
    "    train_or_test (str) either 'train' or 'test'\n",
    "\n",
    "    Outputs:\n",
    "    list of 1 and 0s corresponding to the labels of the training and test sets for the ensemble\n",
    "    '''\n",
    "    with open(outdir + 'sages-level1_out_'+ train_or_test+'_labels.csv') as fo:\n",
    "        for line in fo:\n",
    "            split_line = line.replace('\\n','').split(',')\n",
    "            out = []\n",
    "            for elt in split_line[:-1]:\n",
    "                out.append(float(elt))\n",
    "            return out\n",
    "\n",
    "\n",
    "def tuning_level2(classifiers, cl, outdir, write = False):\n",
    "    '''trains the ensemble and evaluates\n",
    "\n",
    "    Inputs:\n",
    "    classifiers (list of TPOT classifiers)\n",
    "    cl (list of str) classifier labels with the same indexing as classifiers\n",
    "    outdir (str) directory where output files will be saved\n",
    "    write (boolean) for debugging purposes, False will prevent any files from being saved\n",
    "\n",
    "    Outputs:returns None, but will save the following files (if the write variable is True)\n",
    "    classifier name - dataset -level2-tpot_exported_pipeline.py: a python file with the best hyperparameter tuned classifer\n",
    "    level2_summary.csv: performance metrics for all classifiers trained\n",
    "    '''\n",
    "    train_data = pd.read_csv(outdir+'0-level2_train.csv', header=None)\n",
    "    train_data = train_data.transpose()\n",
    "    train_data.drop(train_data.tail(1).index,inplace=True)\n",
    "    Ltrain = get_label_from_l1(outdir, 'train')\n",
    "    if write:\n",
    "        fout0 = open(outdir+'level2_summary.csv', '+a')\n",
    "        fout0.write('RandomSeed,Accuracy,AUROC,F1,Precision,Recall,MCC,Classifier\\n')\n",
    "        fout0.close()\n",
    "\n",
    "    for clf_i in range(len(classifiers)):\n",
    "        clf = classifiers[clf_i]\n",
    "        clf.fit(train_data,np.array(Ltrain))\n",
    "        exctracted_best_model = clf.fitted_pipeline_.steps[-1][1]\n",
    "        if write:\n",
    "            clf.export(outdir+cl[clf_i]+ '-level2-tpot_exported_pipeline.py')\n",
    "\n",
    "        for rs in range(10):\n",
    "            train_data = pd.read_csv(outdir+str(rs)+'-level2_train.csv', header=None)\n",
    "            train_data = train_data.transpose()\n",
    "            test_data = pd.read_csv(outdir+str(rs)+'-level2_test.csv', header=None)\n",
    "            test_data = test_data.transpose()\n",
    "            train_data.drop(train_data.tail(1).index,inplace=True)\n",
    "            test_data.drop(test_data.tail(1).index,inplace=True)\n",
    "            Ltrain = get_label_from_l1(outdir, 'train')\n",
    "            Ltest = get_label_from_l1(outdir, 'test')\n",
    "            rsmodel = exctracted_best_model.fit(train_data,np.array(Ltrain)) \n",
    "            y_predict_test1 = rsmodel.predict(test_data)\n",
    "            acc,aroc,f1_val,precision_val,recall_val,mcc = evaluate(np.array(Ltest), y_predict_test1)\n",
    "            if write:\n",
    "                out_str = str(rs)+','+str(acc)+','+str(aroc)+','+str(f1_val)+','+str(precision_val)+','+str(recall_val) +','+str(mcc) +','+ cl[clf_i] +'\\n'\n",
    "                fout0 = open(outdir+'level2_summary.csv', '+a')\n",
    "                fout0.write(out_str)\n",
    "                fout0.close()\n",
    "\n",
    "\n",
    "def level1_fs2(label_key, outdir):\n",
    "    '''feature selection for the best performing classifiers trained on each of the datasets \n",
    "\n",
    "    Inputs:\n",
    "    label_key (int) column number corresponding to which labels to use in the tox_labels.csv file\n",
    "    outdir (str) directory where output files will be saved\n",
    "\n",
    "    Outputs: returns None, but will save the following files\n",
    "    dataset _variancefs.csv feature selection according to the variance threshold\n",
    "    dataset _genericunifs.csv feature selection using Generic Univariate Selection\n",
    "    dataset _fs.csv feature seleciton using recursive feature elimination with cross-validation using a random forest classifier and a random seed of 0\n",
    "    '''\n",
    "    l = {'sages':{}, 'target':{}, 'fp':{}, 'drug_features':{}}\n",
    "    for rs in range(10):\n",
    "        print(rs)\n",
    "        train, test1, test2, Ltrain, Ltest1, Ltest2 = load_labels(label_key, rs, 0.8)\n",
    "        data = split_norm_data(train, test1, test2)\n",
    "    \n",
    "        for data_set_i in range(len(data)):\n",
    "            data_set = data[data_set_i]\n",
    "            set_name = data_set[0]\n",
    "            if set_name in ['sages','targetsall', 'drug_features']:\n",
    "                print(set_name)\n",
    "                train_set = data_set[1]\n",
    "                \n",
    "                selector = VarianceThreshold()\n",
    "                selector.fit(train_set)\n",
    "                fout = open(outdir+set_name+'_variancefs.csv', 'a+' )\n",
    "                for elt in selector.get_support(indices=False):\n",
    "                    fout.write(str(elt)+ ',')\n",
    "                fout.write('\\n')\n",
    "                fout.close()\n",
    "\n",
    "                transformer = GenericUnivariateSelect(chi2, mode='k_best', param=int(train_set.shape[1]/4))\n",
    "                transformer.fit(train_set, np.array(Ltrain))\n",
    "                fout = open(outdir+set_name+'_genericunifs.csv', 'a+' )\n",
    "                for elt in transformer.get_support(indices=False):\n",
    "                    fout.write(str(elt) + ',')\n",
    "                fout.write('\\n')\n",
    "                fout.close()\n",
    "\n",
    "                selector = RFECV(RandomForestClassifier(random_state=rs), step=1, cv=5)\n",
    "                selector = selector.fit(train_set, np.array(Ltrain))\n",
    "                fout = open(outdir+set_name+'_fs.csv', 'a+' )\n",
    "                for elt in selector.ranking_:\n",
    "                    fout.write(str(elt) + ',')\n",
    "                fout.write('\\n')\n",
    "                fout.close()\n",
    "\n",
    "\n",
    "def get_prroc(prroc_conds, label_key_number, random_seed, train_percent,classifiers, cl, outdir, write=False):\n",
    "    '''calculates the precision recall and receiver operator characteristic curves \n",
    "\n",
    "    Inputs:\n",
    "    prroc_conds (list of str) conatins a list of best classifiers for each dataset \n",
    "    label_key_number (int) column number corresponding to which labels to use in the tox_labels.csv file\n",
    "    random_seed (int) for model instantiation and dataset splitting\n",
    "    train_percent (float) amount of the dataset used for training, the remaning data will be split into two test sets\n",
    "    classifiers (list of TPOT classifiers)\n",
    "    cl (list of str) classifier labels with the same indexing as classifiers\n",
    "    outdir (str) directory where output files will be saved\n",
    "    write (boolean) for debugging purposes, False will prevent any files from being saved\n",
    "\n",
    "    Outputs: returns None, but will save the following files (if the write variable is True)\n",
    "    random seed dataset - classifier -curves.csv file containing false positive rate, true positive rate, precision and recall for classifier traind on the random seed\n",
    "    '''\n",
    "    train, test1, test2, Ltrain, Ltest1, Ltest2 = load_labels(label_key_number, random_seed, train_percent)\n",
    "    data = split_norm_data(train, test1, test2)\n",
    "    \n",
    "    for data_set_i in range(len(data)):\n",
    "        print('****************')\n",
    "        data_set = data[data_set_i]\n",
    "        print(data_set[0])\n",
    "        train_set = data_set[1]\n",
    "        test_set1 = data_set[2]\n",
    "        test_set2 = data_set[3]\n",
    "        \n",
    "        for clf_i in range(len(classifiers)):\n",
    "            clf = classifiers[clf_i]\n",
    "            if data_set[0]+ '-' + cl[clf_i] in prroc_conds:\n",
    "                for rs in range(random_seed, random_seed +10):\n",
    "                    if rs == random_seed:\n",
    "                        clf.fit(train_set,np.array(Ltrain))\n",
    "                        exctracted_best_model = clf.fitted_pipeline_.steps[-1][1]\n",
    "                    \n",
    "                    rstrain, rstest1, rstest2, rsLtrain, rsLtest1, rsLtest2 = load_labels(label_key_number, rs, train_percent)\n",
    "                    rsdata = split_norm_data(rstrain, rstest1, rstest2)\n",
    "                    rstrain_set = rsdata[data_set_i][1]\n",
    "                    rstest_set1 = rsdata[data_set_i][2]\n",
    "                    rstest_set2 = rsdata[data_set_i][3]\n",
    "                    rsmodel = exctracted_best_model.fit(rstrain_set,np.array(rsLtrain))\n",
    "                    y_predict_test1 = rsmodel.predict_proba(rstest_set1)\n",
    "                    y_predict_test2 = rsmodel.predict_proba(rstest_set2)\n",
    "                    if write:\n",
    "                        temp_ypred = []\n",
    "                        for elt in list(y_predict_test2):\n",
    "                            temp_ypred.append(float(elt[1]))\n",
    "                        fpr, tpr, thresholds = roc_curve(np.array(rsLtest2), temp_ypred, pos_label=1)\n",
    "                        precision, recall, thresholds = precision_recall_curve(np.array(rsLtest1), temp_ypred)\n",
    "                        filename = outdir + 'prroc/'+str(rs)+ data_set[0]+'-'+cl[clf_i]+'-curves.csv'\n",
    "                        with open(filename,\"w\") as f:\n",
    "                            f.write(\"\\n\".join(\",\".join(map(str, x)) for x in (fpr,tpr,precision, recall)))\n",
    "    \n",
    "\n",
    "def split_norm_data_dict(train, test1, test2):\n",
    "    '''loads the data and normalizes by the training set\n",
    "\n",
    "    Inputs: train, test1, test2 each (list) contains names of the drugs (str) in the data subset \n",
    "\n",
    "    Output:\n",
    "    dictionary where the key is the dataset label (str) and the values are a tuple of pandas dataframes corresponding to train_set,test_set1,test_set2\n",
    "\n",
    "    '''\n",
    "    load_train = load_data(train)\n",
    "    load_test1 = load_data(test1) \n",
    "    load_test2 = load_data(test2)\n",
    "    l = ['sages', 'fp', 'drug_features', 'targetsall']\n",
    "\n",
    "    out = {}\n",
    "    for data_i in range(len(load_train)):\n",
    "        train_set = norm_data_by_train(load_train[data_i],load_train[data_i])\n",
    "        test_set1 = norm_data_by_train(load_train[data_i],load_test1[data_i])\n",
    "        test_set2 = norm_data_by_train(load_train[data_i],load_test2[data_i])\n",
    "        out[l[data_i]]= (train_set,test_set1,test_set2)\n",
    "    return out\n",
    "\n",
    "def load_drugs_to_pred_sub(csvfile):\n",
    "    '''loads the drugs currently in clinical trials which the model will classify along with one of the feature set types\n",
    "\n",
    "    Inputs:\n",
    "    csvfile (str) name of the file containing the clinical trial drug information\n",
    "\n",
    "    Outputs:\n",
    "    label of drug names (list of str)\n",
    "    drug features (pandas dataframe)\n",
    "    '''\n",
    "    label = []\n",
    "    data = []\n",
    "    with open(csvfile) as fo:\n",
    "        for line in fo:\n",
    "            split_line = line.replace('\\n','').split(',')\n",
    "            label.append(split_line[0])\n",
    "            temp = []\n",
    "            for elt in split_line[1:]:\n",
    "                temp.append(float(elt))\n",
    "            data.append(temp)\n",
    "    return label, pd.DataFrame(data)\n",
    "\n",
    "def load_drugs_to_pred():\n",
    "    '''loads all of the clinical trial drugs and their data sets \n",
    "\n",
    "    Inputs: None\n",
    "\n",
    "    Outputs:dictionary where the key is the feature type (str) and the value is the drug data matrix data (pandas dataframe)\n",
    "    '''\n",
    "    out = {}\n",
    "    file_types = ['sages','fp','targetsall','drug_features']\n",
    "    for name in file_types:\n",
    "        label, data = load_drugs_to_pred_sub('trials_'+name+'.csv')\n",
    "        out[name] = data\n",
    "    return out,label\n",
    "\n",
    "def load_level2_drugspred(outdir,rs):\n",
    "    '''loads the outputs from the individual classifiers to use as training for the ensemble\n",
    "\n",
    "    Inputs: \n",
    "    outdir (str) directory of the files to load\n",
    "    rs (int) random seed for determining which subset of the data to use\n",
    "\n",
    "    Outputs: train data (pandas dataframe) of \n",
    "    '''\n",
    "    train_data = pd.read_csv(outdir+str(rs)+'predtrialdrugs-level2.csv', header=None)\n",
    "    train_data = train_data.transpose()\n",
    "    train_data.drop(train_data.head(1).index,inplace=True)\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def pred_trials_level1(pred_conds, label_key_number, train_percent,classifiers, cl, outdir, write=False):\n",
    "    '''individual classifier (trained on the test set) predictions of drugs in clinical trials\n",
    "\n",
    "    Inputs:\n",
    "    pred_conds (list of str) classifier types that performed the best for each feature type set\n",
    "    label_key_number (int) column number corresponding to which labels to use in the tox_labels.csv file\n",
    "    train_percent (float) amount of the dataset used for training, the remaning data will be split into two test sets\n",
    "    classifiers (list of TPOT classifiers)\n",
    "    cl (list of str) classifier labels with the same indexing as classifiers\n",
    "    outdir (str) directory where output files will be saved\n",
    "    write (boolean) for debugging purposes, False will prevent any files from being saved\n",
    "    \n",
    "    Outputs: returns None, but will save the following files (if the write variable is True)\n",
    "    random seed predtrialdrugs-level2.csv - saves the predictions of the individual classifiers for use in the ensemble predictor\n",
    "    '''\n",
    "    pred_drugs_data_dict, pred_drug_names = load_drugs_to_pred()\n",
    "    for clf_i in range(len(classifiers)):\n",
    "        for data_info in pred_conds:\n",
    "            data_name = data_info.split('-')[0]\n",
    "            if data_name+ '-' + cl[clf_i] in pred_conds:\n",
    "                print('****************')\n",
    "                print(data_name)\n",
    "                train, test1, test2, Ltrain, Ltest1, Ltest2 = load_labels(label_key_number, 0, train_percent)\n",
    "                train_set,test_set1,test_set2 = split_norm_data_dict(train, test1, test2)[data_name]\n",
    "                clf = classifiers[clf_i]\n",
    "                clf.fit(train_set,np.array(Ltrain))\n",
    "                exctracted_best_model = clf.fitted_pipeline_.steps[-1][1]\n",
    "                for rs in range(10):\n",
    "                    train, test1, test2, Ltrain, Ltest1, Ltest2 = load_labels(label_key_number, rs, train_percent)\n",
    "                    train_set,test_set1,test_set2 = split_norm_data_dict(train, test1, test2)[data_name]\n",
    "                    rsmodel = exctracted_best_model.fit(train_set,np.array(Ltrain))\n",
    "                    try:\n",
    "                        y_predict = rsmodel.predict_proba(pred_drugs_data_dict[data_name])\n",
    "                        if write:\n",
    "                            fout4= open(outdir+str(rs)+'predtrialdrugs-level2.csv','a+')\n",
    "                            fout4.write(str(data_name))\n",
    "                            for elt in list(y_predict):\n",
    "                                fout4.write(',' + str(elt[0]))\n",
    "                            fout4.write('\\n')\n",
    "                            fout4.close()\n",
    "                    except:\n",
    "                        y_predict = rsmodel.predict(pred_drugs_data_dict[data_name])\n",
    "                        if write:\n",
    "                            fout4= open(outdir+str(rs)+'predtrialdrugs-level2.csv','a+')\n",
    "                            fout4.write(str(data_name))\n",
    "                            for elt in list(y_predict):\n",
    "                                fout4.write(','+str(elt))\n",
    "                            fout4.write('\\n')\n",
    "                            fout4.close()\n",
    "\n",
    "def pred_trials_level2(outdir,write=False):\n",
    "    '''ensemble predictor trained on test set for use predicting drugs in clinical trials\n",
    "    \n",
    "    Inputs: \n",
    "    outdir (str) directory of where to save the written files \n",
    "    write (boolean) for debugging purposes, False will prevent any files from being saved\n",
    "\n",
    "    Outputs:returns None, but will save the following files (if the write variable is True)\n",
    "    final_predictions_predtrialdrugs.csv - each column represents \n",
    "    '''\n",
    "    pred_drugs_data_dict, pred_drug_names = load_drugs_to_pred()\n",
    "    if write:\n",
    "        fout4= open(outdir+'final_predictions_predtrialdrugs.csv','a+')\n",
    "        fout4.write(','.join(pred_drug_names) +'\\n')\n",
    "        fout4.close()\n",
    "    for rs in range(10):\n",
    "        for clf_i in range(len(classifiers)):\n",
    "            if cl[clf_i]=='tpotsk':\n",
    "                if rs == 0:\n",
    "                    train_data = pd.read_csv(outdir+'0-level2_train.csv', header=None)\n",
    "                    train_data = train_data.transpose()\n",
    "                    train_data.drop(train_data.tail(1).index,inplace=True)\n",
    "                    Ltrain = get_label_from_l1(outdir, 'train')\n",
    "                    clf = classifiers[clf_i]\n",
    "                    clf.fit(train_data,np.array(Ltrain))\n",
    "                    exctracted_best_model = clf.fitted_pipeline_.steps[-1][1]\n",
    "                    y_predict = exctracted_best_model.predict(train_data)\n",
    "                \n",
    "                train_data = pd.read_csv(outdir+str(0)+'-level2_train.csv', header=None)\n",
    "                train_data = train_data.transpose()\n",
    "                train_data.drop(train_data.tail(1).index,inplace=True)\n",
    "                rsmodel = exctracted_best_model.fit(train_data,np.array(Ltrain))\n",
    "                level2_drugstopred_data = load_level2_drugspred(outdir,rs)\n",
    "                y_predict_test1 = rsmodel.predict(level2_drugstopred_data)\n",
    "                if write:\n",
    "                    fout4= open(outdir+'final_predictions_predtrialdrugs.csv','a+')\n",
    "                    for elt in list(y_predict_test1):\n",
    "                        fout4.write(str(elt)+ ',')\n",
    "                    fout4.write('\\n')\n",
    "                    fout4.close()\n",
    "\n",
    "def get_prroc_averages(prroc_conds,classifiers, cl, outdir, write=False):\n",
    "    ''' averages the classifier performance for all cross validation steps\n",
    "\n",
    "    Inputs:\n",
    "    prroc_conds (list of str) the results from which classifier and feature set combo to average\n",
    "    outdir (str) directory of where to save the written files \n",
    "    classifiers (list of TPOT classifiers)\n",
    "    cl (list of str) classifier labels with the same indexing as classifiers\n",
    "    write (boolean) for debugging purposes, False will prevent any files from being saved\n",
    "\n",
    "    Outputs:returns None, but will save the following files (if the write variable is True)\n",
    "    newprroc/level2_summary.csv average performance metrics for the hyperparameter tuned enesemble classifiers\n",
    "    '''\n",
    "    for clf_i in range(len(classifiers)):\n",
    "        if 'all-'+cl[clf_i] in prroc_conds:\n",
    "            for rs in range(10):\n",
    "                train_data = pd.read_csv(outdir+str(rs)+'-level2_train.csv', header=None)\n",
    "                train_data = train_data.transpose()\n",
    "                train_data.drop(train_data.tail(1).index,inplace=True)\n",
    "                Ltrain = get_label_from_l1(outdir, 'train')\n",
    "                test_data = pd.read_csv(outdir+str(rs)+'-level2_test.csv', header=None)\n",
    "                average_predictor = test_data.mean().to_list()[:-1]\n",
    "                test_data = test_data.transpose()\n",
    "                test_data.drop(test_data.tail(1).index,inplace=True)\n",
    "                Ltest = get_label_from_l1(outdir, 'test')\n",
    "                \n",
    "                if rs == 0:\n",
    "                    clf = classifiers[clf_i]\n",
    "                    clf.fit(train_data,np.array(Ltrain))\n",
    "                    rsmodel = clf.fitted_pipeline_.steps[-1][1]\n",
    "\n",
    "                y_predict_test1 = rsmodel.predict_proba(test_data)\n",
    "                temp_average_predictor = []\n",
    "                for elt in average_predictor:\n",
    "                    if elt <=0.5:\n",
    "                        temp_average_predictor.append(1)\n",
    "                    else:\n",
    "                        temp_average_predictor.append(0)\n",
    "                acc,aroc,f1_val,precision_val,recall_val,mcc = evaluate(np.array(Ltest), temp_average_predictor)\n",
    "                if write:\n",
    "                    out_str = str(rs)+','+str(acc)+','+str(aroc)+','+str(f1_val)+','+str(precision_val)+','+str(recall_val) +','+str(mcc) +',allaverage\\n'\n",
    "                    fout0 = open(outdir+'newprroc/level2_summary.csv', '+a')\n",
    "                    fout0.write(out_str)\n",
    "                    fout0.close()\n",
    "\n",
    "                    temp_ypred = []\n",
    "                    for elt in list(y_predict_test1):\n",
    "                        temp_ypred.append(float(elt[1]))\n",
    "                    fpr, tpr, thresholds = roc_curve(np.array(Ltest), temp_ypred, pos_label=1)\n",
    "                    precision, recall, thresholds = precision_recall_curve(np.array(Ltest), temp_ypred)\n",
    "\n",
    "                    filename = outdir + 'newprroc/'+str(rs)+'all-'+cl[clf_i]+'-curves.csv'\n",
    "                    with open(filename,\"w\") as f:\n",
    "                        f.write(\"\\n\".join(\",\".join(map(str, x)) for x in (fpr,tpr,precision, recall)))\n",
    "                    \n",
    "                    fpr, tpr, thresholds = roc_curve(np.array(Ltest), average_predictor, pos_label=1)\n",
    "                    precision, recall, thresholds = precision_recall_curve(np.array(Ltest), average_predictor)\n",
    "\n",
    "                    filename = outdir +'newprroc/'+ str(rs)+'all-average-curves.csv'\n",
    "                    with open(filename,\"w\") as f:\n",
    "                        f.write(\"\\n\".join(\",\".join(map(str, x)) for x in (fpr,tpr,precision, recall)))\n",
    "\n",
    "def get_average_performance_l1(outdir, infile,outfile):\n",
    "    '''averages the classifier performance for all cross validation steps for the classifiers trained on each feature set\n",
    "\n",
    "    Inputs:\n",
    "    outdir (str) directory of where to save the written files \n",
    "    infile (str) the file that contains all the performance metrics for the model\n",
    "    outfile (str) name of the file to save the average performance to\n",
    "\n",
    "    Output:returns None, but will save the following file  - outfile\n",
    "    '''\n",
    "    out = {}\n",
    "    line_i = 0\n",
    "    with open(outdir+infile) as fo:\n",
    "        for line in fo:\n",
    "            if line_i != 0:\n",
    "                split_line = line[:-1].split(',')\n",
    "                current_key = split_line[1] + '_' + split_line[8]\n",
    "                if current_key not in out:\n",
    "                    out[current_key] = {'Accuracy':0, 'AUROC':0,'F1':0,'Precision':0,'Recall':0, 'MCC':0}\n",
    "                out[current_key]['Accuracy'] = out[current_key]['Accuracy'] + float(split_line[2])\n",
    "                out[current_key]['AUROC'] = out[current_key]['AUROC'] + float(split_line[3])\n",
    "                out[current_key]['F1'] = out[current_key]['F1'] + float(split_line[4])\n",
    "                out[current_key]['Precision'] = out[current_key]['Precision'] + float(split_line[5])\n",
    "                out[current_key]['Recall'] = out[current_key]['Recall'] + float(split_line[6])\n",
    "                out[current_key]['MCC'] = out[current_key]['MCC'] + float(split_line[7])\n",
    "            line_i += 1\n",
    "    l = ['Accuracy', 'AUROC','F1','Precision','Recall','MCC']\n",
    "    for dict_key in out.keys():\n",
    "        line_out = dict_key\n",
    "        for k in l:\n",
    "            line_out = line_out + ',' + str(out[dict_key][k]/10)\n",
    "        fout = open(outdir+outfile, 'a+')\n",
    "        fout.write(line_out + '\\n')\n",
    "        fout.close()\n",
    "\n",
    "def make_level2_data(outdir,test_or_train, best_models):\n",
    "    '''reformats the predictions from the classifiers trained on a subset of features to be used as training and testing data for the ensemble\n",
    "\n",
    "    Inputs:\n",
    "    outdir (str) directory of where to save the written files \n",
    "    test_or_train (str) 'test' or 'train' label indicating which dataset to transpose\n",
    "    best_models (str) represents the label of the output of the classifier trained on a certain feature subset to transpose\n",
    "\n",
    "    Outputs:returns None, but will save the following file\n",
    "    random seed -level2_ test_or_train .csv - the data for use in training the ensemble\n",
    "    '''\n",
    "    for rs in range(10):\n",
    "        out = ''\n",
    "        for bm in best_models:\n",
    "            with open(outdir+ bm+ '-' + str(rs)+ '-level2_'+test_or_train+'.csv') as fo:\n",
    "                for line in fo:\n",
    "                    out = out + line\n",
    "        fout = open(outdir+ str(rs)+'-level2_'+test_or_train+'.csv','a+')\n",
    "        fout.write(out)\n",
    "        fout.close()\n",
    "\n",
    "def get_average_performance_l2(outdir, infile,outfile):\n",
    "    '''averages the ensemble performance for all cross validation steps\n",
    "\n",
    "    Inputs:\n",
    "    outdir (str) directory of where to save the written files \n",
    "    infile (str) the file that contains all the performance metrics for the model\n",
    "    outfile (str) name of the file to save the average performance to\n",
    "\n",
    "    Output:returns None, but will save the following file  - outfile\n",
    "    '''\n",
    "    out = {}\n",
    "    with open(outdir+infile) as fo:\n",
    "        line_i = 0\n",
    "        for line in fo:\n",
    "            if line_i != 0:\n",
    "                split_line = line.replace('\\n','').split(',')\n",
    "                current_key = split_line[7]\n",
    "                if current_key not in out:\n",
    "                    out[current_key] = {'Accuracy':0, 'AUROC':0,'F1':0,'Precision':0,'Recall':0, 'MCC':0}\n",
    "                out[current_key]['Accuracy'] = out[current_key]['Accuracy'] + float(split_line[1])\n",
    "                out[current_key]['AUROC'] = out[current_key]['AUROC'] + float(split_line[2])\n",
    "                out[current_key]['F1'] = out[current_key]['F1'] + float(split_line[3])\n",
    "                out[current_key]['Precision'] = out[current_key]['Precision'] + float(split_line[4])\n",
    "                out[current_key]['Recall'] = out[current_key]['Recall'] + float(split_line[5])\n",
    "                out[current_key]['MCC'] = out[current_key]['MCC'] + float(split_line[6])\n",
    "            line_i+=1\n",
    "    l = ['Accuracy', 'AUROC','F1','Precision','Recall','MCC']\n",
    "    for dict_key in out.keys():\n",
    "        line_out = dict_key\n",
    "        for k in l:\n",
    "            line_out = line_out + ',' + str(out[dict_key][k]/10)\n",
    "        fout = open(outdir+outfile, 'a+')\n",
    "        fout.write(line_out + '\\n')\n",
    "        fout.close()\n",
    "\n",
    "# Variable Values\n",
    "parametersRF = {'criterion': ['entropy', 'gini'],'max_depth': list(np.linspace(10, 500, 10, dtype = int)) + [None],'max_features': ['auto', 'sqrt','log2', None],'min_samples_leaf': [2, 15],'min_samples_split': [5, 15],'n_estimators': list(np.linspace(150, 500, 10, dtype = int))}\n",
    "parametersMLP = {'activation': ['identity', 'loginst', 'tanh','relu'],'hidden_layer_sizes': list(np.linspace(25,400, 10, dtype = int)),'solver': ['lbfgs', 'sgd','adam']}\n",
    "parametersXGB = {'max_depth': list(np.linspace(10, 500, 10, dtype = int)) + [None],'n_estimators': list(np.linspace(150, 500, 10, dtype = int))}\n",
    "my_search = TPOTClassifier( population_size= 24, offspring_size= 12, verbosity= 2, early_stop= 12, scoring = 'accuracy', cv = 5, generations= 5,random_state=0,\n",
    "                        config_dict={'sklearn.ensemble.RandomForestClassifier': parametersRF,\n",
    "                            'sklearn.neural_network.MLPClassifier': parametersMLP,\n",
    "                            'xgboost.XGBClassifier':parametersXGB\n",
    "                                })\n",
    "og_search = TPOTClassifier(generations= 5, population_size= 24, offspring_size= 12, verbosity= 2, early_stop= 12, config_dict='TPOT NN', cv = 5, scoring = 'accuracy', random_state=0,)\n",
    "classifiers = [my_search, og_search]\n",
    "cl = ['tpotsk','tpotdefault']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d3b1c7b-700d-4af6-a2b9-325d27d92310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tpot==0.11.1\n",
      "  Downloading TPOT-0.11.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: numpy>=1.16.3 in /opt/anaconda3/lib/python3.11/site-packages (from tpot==0.11.1) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from tpot==0.11.1) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from tpot==0.11.1) (1.7.2)\n",
      "Requirement already satisfied: deap>=1.2 in /opt/anaconda3/lib/python3.11/site-packages (from tpot==0.11.1) (1.4.3)\n",
      "Requirement already satisfied: update-checker>=0.16 in /opt/anaconda3/lib/python3.11/site-packages (from tpot==0.11.1) (0.18.0)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in /opt/anaconda3/lib/python3.11/site-packages (from tpot==0.11.1) (4.67.1)\n",
      "Requirement already satisfied: stopit>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from tpot==0.11.1) (1.1.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /opt/anaconda3/lib/python3.11/site-packages (from tpot==0.11.1) (2.3.3)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /opt/anaconda3/lib/python3.11/site-packages (from tpot==0.11.1) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=0.24.2->tpot==0.11.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=0.24.2->tpot==0.11.1) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=0.24.2->tpot==0.11.1) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=0.22.0->tpot==0.11.1) (3.6.0)\n",
      "Requirement already satisfied: requests>=2.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from update-checker>=0.16->tpot==0.11.1) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=0.24.2->tpot==0.11.1) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot==0.11.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot==0.11.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot==0.11.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot==0.11.1) (2024.8.30)\n",
      "Downloading TPOT-0.11.1-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tpot\n",
      "Successfully installed tpot-0.11.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tpot==0.11.1 --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4624df3-3d42-463b-a022-8ca8f0411030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.1.dev9+g1bca6c6a5\n"
     ]
    }
   ],
   "source": [
    "import tpot\n",
    "print(tpot.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aae3426e-43d6-44ec-8d5f-496f29c0a121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************\n",
      "sages\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TPOTEstimator.__init__() got an unexpected keyword argument 'offspring_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m\n\u001b[1;32m      4\u001b[0m classifiers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     TPOTClassifier(\n\u001b[1;32m      6\u001b[0m         generations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     ),\n\u001b[1;32m     15\u001b[0m ]\n\u001b[1;32m     17\u001b[0m cl \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtpot\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 19\u001b[0m tuning_level1(\n\u001b[1;32m     20\u001b[0m     label_key_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     21\u001b[0m     random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     22\u001b[0m     train_percent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m     23\u001b[0m     classifiers\u001b[38;5;241m=\u001b[39mclassifiers,\n\u001b[1;32m     24\u001b[0m     cl\u001b[38;5;241m=\u001b[39mcl,\n\u001b[1;32m     25\u001b[0m     outdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     26\u001b[0m     write\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/withDRAWN/tox_pred.py:189\u001b[0m, in \u001b[0;36mtuning_level1\u001b[0;34m(label_key_number, random_seed, train_percent, classifiers, cl, outdir, write)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clf_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(classifiers)):\n\u001b[1;32m    188\u001b[0m     clf \u001b[38;5;241m=\u001b[39m classifiers[clf_i]\n\u001b[0;32m--> 189\u001b[0m     clf\u001b[38;5;241m.\u001b[39mfit(train_set,np\u001b[38;5;241m.\u001b[39marray(Ltrain))\n\u001b[1;32m    190\u001b[0m     exctracted_best_model \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mfitted_pipeline_\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m write:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tpot/tpot_estimator/templates/tpottemplates.py:540\u001b[0m, in \u001b[0;36mTPOTClassifier.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    532\u001b[0m     get_search_space_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y)), \n\u001b[1;32m    533\u001b[0m                                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28mlen\u001b[39m(y), \n\u001b[1;32m    534\u001b[0m                                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features\u001b[39m\u001b[38;5;124m\"\u001b[39m:X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m    535\u001b[0m                                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state}\n\u001b[1;32m    537\u001b[0m     search_space \u001b[38;5;241m=\u001b[39m get_template_search_spaces(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_space, classification\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inner_predictors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_inner_classifiers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mget_search_space_params)\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28msuper\u001b[39m(TPOTClassifier,\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    541\u001b[0m         search_space\u001b[38;5;241m=\u001b[39msearch_space,\n\u001b[1;32m    542\u001b[0m         scorers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscorers, \n\u001b[1;32m    543\u001b[0m         scorers_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscorers_weights,\n\u001b[1;32m    544\u001b[0m         cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv,\n\u001b[1;32m    545\u001b[0m         other_objective_functions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mother_objective_functions, \u001b[38;5;66;03m#tpot.objectives.estimator_objective_functions.number_of_nodes_objective],\u001b[39;00m\n\u001b[1;32m    546\u001b[0m         other_objective_functions_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mother_objective_functions_weights,\n\u001b[1;32m    547\u001b[0m         objective_function_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective_function_names,\n\u001b[1;32m    548\u001b[0m         bigger_is_better \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbigger_is_better,\n\u001b[1;32m    549\u001b[0m         categorical_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_features,\n\u001b[1;32m    550\u001b[0m         memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory,\n\u001b[1;32m    551\u001b[0m         preprocessing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessing,\n\u001b[1;32m    552\u001b[0m         max_time_mins\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_time_mins, \n\u001b[1;32m    553\u001b[0m         max_eval_time_mins\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_eval_time_mins, \n\u001b[1;32m    554\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    555\u001b[0m         validation_strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_strategy,\n\u001b[1;32m    556\u001b[0m         validation_fraction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_fraction, \n\u001b[1;32m    557\u001b[0m         early_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stop,\n\u001b[1;32m    558\u001b[0m         warm_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start,\n\u001b[1;32m    559\u001b[0m         periodic_checkpoint_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperiodic_checkpoint_folder, \n\u001b[1;32m    560\u001b[0m         verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    561\u001b[0m         classification\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    562\u001b[0m         memory_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_limit,\n\u001b[1;32m    563\u001b[0m         client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient,\n\u001b[1;32m    564\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state,\n\u001b[1;32m    565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtpotestimator_kwargs)\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X,y)\n",
      "\u001b[0;31mTypeError\u001b[0m: TPOTEstimator.__init__() got an unexpected keyword argument 'offspring_size'"
     ]
    }
   ],
   "source": [
    "from tox_pred import tuning_level1\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "classifiers = [\n",
    "    TPOTClassifier(\n",
    "        generations=5,\n",
    "        population_size=24,\n",
    "        offspring_size=12,\n",
    "        verbosity=2,\n",
    "        early_stop=12,\n",
    "        scoring='accuracy',\n",
    "        cv=5,\n",
    "        random_state=0\n",
    "    ),\n",
    "]\n",
    "\n",
    "cl = ['tpot']\n",
    "\n",
    "tuning_level1(\n",
    "    label_key_number=1,\n",
    "    random_seed=0,\n",
    "    train_percent=0.7,\n",
    "    classifiers=classifiers,\n",
    "    cl=cl,\n",
    "    outdir='./results/',\n",
    "    write=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a16e14e-a7b1-490d-b5a9-864dc674e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "637aa884-d085-4743-953b-1423a3876309",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_info_df = pd.read_csv(\"drug_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2438a09-2ec5-4479-a493-cd1a8204d653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>atc</th>\n",
       "      <th>chirality</th>\n",
       "      <th>alogp</th>\n",
       "      <th>aromatic rings</th>\n",
       "      <th>cx_logd</th>\n",
       "      <th>cx_logp</th>\n",
       "      <th>cx_most_apka</th>\n",
       "      <th>cx_most_bpka</th>\n",
       "      <th>full_mwt</th>\n",
       "      <th>...</th>\n",
       "      <th>smiles</th>\n",
       "      <th>name</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "      <th>Unnamed: 33</th>\n",
       "      <th>Unnamed: 34</th>\n",
       "      <th>Unnamed: 35</th>\n",
       "      <th>Unnamed: 36</th>\n",
       "      <th>Unnamed: 37</th>\n",
       "      <th>Unnamed: 38</th>\n",
       "      <th>Unnamed: 39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL2</td>\n",
       "      <td>C02CA01_</td>\n",
       "      <td>2</td>\n",
       "      <td>1.78</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.43</td>\n",
       "      <td>1.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.24</td>\n",
       "      <td>383.41</td>\n",
       "      <td>...</td>\n",
       "      <td>COc1cc2nc(N3CCN(C(=O)c4ccco4)CC3)nc(N)c2cc1OC</td>\n",
       "      <td>Prazosin_CP-122991_CP-12299_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3</td>\n",
       "      <td>N07BA01_</td>\n",
       "      <td>1</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.58</td>\n",
       "      <td>162.24</td>\n",
       "      <td>...</td>\n",
       "      <td>CN1CCC[C@H]1c1cccnc1</td>\n",
       "      <td>Nicoderm cq_Stoppers_Nicotrol_Nicotrol NS_Niqu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4</td>\n",
       "      <td>J01MA01_S02AA16_S01AE01_</td>\n",
       "      <td>0</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.51</td>\n",
       "      <td>5.29</td>\n",
       "      <td>6.16</td>\n",
       "      <td>361.37</td>\n",
       "      <td>...</td>\n",
       "      <td>CC1COc2c(N3CCN(C)CC3)c(F)cc3c(=O)c(C(=O)O)cn1c23</td>\n",
       "      <td>Ocuflox_Visiren_Tarivid_DL-8280_Exocin_HOE 280...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL5</td>\n",
       "      <td>J01MB02_</td>\n",
       "      <td>2</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.79</td>\n",
       "      <td>4.37</td>\n",
       "      <td>6.06</td>\n",
       "      <td>232.24</td>\n",
       "      <td>...</td>\n",
       "      <td>CCn1cc(C(=O)O)c(=O)c2ccc(C)nc21</td>\n",
       "      <td>Negram_Nalidixane_Uriben_Nalidixic acid_Wintom...</td>\n",
       "      <td>320_Nalidixate_Mictral_NSC-82174_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL6</td>\n",
       "      <td>C01EB03_M02AA23_M01AB51_S01BC01_M01AB01_</td>\n",
       "      <td>2</td>\n",
       "      <td>3.93</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>357.79</td>\n",
       "      <td>...</td>\n",
       "      <td>COc1ccc2c(c1)c(CC(=O)O)c(C)n2C(=O)c1ccc(Cl)cc1</td>\n",
       "      <td>Indocid ret_NSC-757061_Indoderm_Rheumacin la_I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539</th>\n",
       "      <td>CHEMBL1533310</td>\n",
       "      <td>L03AX14_</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.85</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.58</td>\n",
       "      <td>184.07</td>\n",
       "      <td>...</td>\n",
       "      <td>Cl.Cl.NCCc1c[nH]cn1</td>\n",
       "      <td>Histamine dihydrocloride_Histamine dihydrochlo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>CHEMBL1534525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.27</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>5.16</td>\n",
       "      <td>12.54</td>\n",
       "      <td>9.82</td>\n",
       "      <td>568.56</td>\n",
       "      <td>...</td>\n",
       "      <td>COCC(=O)O[C@]1(CCN(C)CCCc2nc3ccccc3[nH]2)CCc2c...</td>\n",
       "      <td>RO 40-5967/001_Mibefradil hydrochloride_Posico...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541</th>\n",
       "      <td>CHEMBL1560089</td>\n",
       "      <td>C04AD02_</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.32</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.86</td>\n",
       "      <td>434.45</td>\n",
       "      <td>...</td>\n",
       "      <td>CN(CCO)CC(O)Cn1cnc2c1c(=O)n(C)c(=O)n2C.O=C(O)c...</td>\n",
       "      <td>Xanthinol nicotinate_Complamin ret_Complamin_X...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542</th>\n",
       "      <td>CHEMBL1562610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>4.74</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.82</td>\n",
       "      <td>6.09</td>\n",
       "      <td>3.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>336.15</td>\n",
       "      <td>...</td>\n",
       "      <td>Cc1ccc(Cl)c(Nc2ccccc2C(=O)[O-])c1Cl.O.[Na+]</td>\n",
       "      <td>Meclofenamate sodium hydrate_Sodium meclofenam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>CHEMBL1565476</td>\n",
       "      <td>M01AX04_</td>\n",
       "      <td>0</td>\n",
       "      <td>2.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2.47</td>\n",
       "      <td>5.49</td>\n",
       "      <td>4.30</td>\n",
       "      <td>300.36</td>\n",
       "      <td>...</td>\n",
       "      <td>CCCC1C(=O)N2C(N(C)C)=Nc3ccc(C)cc3N2C1=O</td>\n",
       "      <td>Apazone dihydrate_Azapropazone_Rheumox 600_Cin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2544 rows  40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0                                       atc  chirality  \\\n",
       "0           CHEMBL2                                  C02CA01_          2   \n",
       "1           CHEMBL3                                  N07BA01_          1   \n",
       "2           CHEMBL4                  J01MA01_S02AA16_S01AE01_          0   \n",
       "3           CHEMBL5                                  J01MB02_          2   \n",
       "4           CHEMBL6  C01EB03_M02AA23_M01AB51_S01BC01_M01AB01_          2   \n",
       "...             ...                                       ...        ...   \n",
       "2539  CHEMBL1533310                                  L03AX14_          2   \n",
       "2540  CHEMBL1534525                                       NaN          1   \n",
       "2541  CHEMBL1560089                                  C04AD02_          0   \n",
       "2542  CHEMBL1562610                                       NaN          2   \n",
       "2543  CHEMBL1565476                                  M01AX04_          0   \n",
       "\n",
       "      alogp  aromatic rings  cx_logd  cx_logp  cx_most_apka  cx_most_bpka  \\\n",
       "0      1.78             3.0     1.43     1.65           NaN          7.24   \n",
       "1      1.85             1.0    -0.04     1.16           NaN          8.58   \n",
       "2      1.54             2.0    -0.47     0.51          5.29          6.16   \n",
       "3      1.42             2.0    -0.45     0.79          4.37          6.06   \n",
       "4      3.93             3.0     0.26     3.53          3.79           NaN   \n",
       "...     ...             ...      ...      ...           ...           ...   \n",
       "2539  -0.09             1.0    -2.85    -0.70           NaN          9.58   \n",
       "2540   5.27             3.0     2.75     5.16         12.54          9.82   \n",
       "2541  -2.28             2.0    -3.32    -1.85           NaN          8.86   \n",
       "2542   4.74             2.0     2.82     6.09          3.79           NaN   \n",
       "2543   2.06             1.0     0.96     2.47          5.49          4.30   \n",
       "\n",
       "      full_mwt  ...                                             smiles  \\\n",
       "0       383.41  ...      COc1cc2nc(N3CCN(C(=O)c4ccco4)CC3)nc(N)c2cc1OC   \n",
       "1       162.24  ...                               CN1CCC[C@H]1c1cccnc1   \n",
       "2       361.37  ...   CC1COc2c(N3CCN(C)CC3)c(F)cc3c(=O)c(C(=O)O)cn1c23   \n",
       "3       232.24  ...                    CCn1cc(C(=O)O)c(=O)c2ccc(C)nc21   \n",
       "4       357.79  ...     COc1ccc2c(c1)c(CC(=O)O)c(C)n2C(=O)c1ccc(Cl)cc1   \n",
       "...        ...  ...                                                ...   \n",
       "2539    184.07  ...                                Cl.Cl.NCCc1c[nH]cn1   \n",
       "2540    568.56  ...  COCC(=O)O[C@]1(CCN(C)CCCc2nc3ccccc3[nH]2)CCc2c...   \n",
       "2541    434.45  ...  CN(CCO)CC(O)Cn1cnc2c1c(=O)n(C)c(=O)n2C.O=C(O)c...   \n",
       "2542    336.15  ...        Cc1ccc(Cl)c(Nc2ccccc2C(=O)[O-])c1Cl.O.[Na+]   \n",
       "2543    300.36  ...            CCCC1C(=O)N2C(N(C)C)=Nc3ccc(C)cc3N2C1=O   \n",
       "\n",
       "                                                   name  \\\n",
       "0                          Prazosin_CP-122991_CP-12299_   \n",
       "1     Nicoderm cq_Stoppers_Nicotrol_Nicotrol NS_Niqu...   \n",
       "2     Ocuflox_Visiren_Tarivid_DL-8280_Exocin_HOE 280...   \n",
       "3     Negram_Nalidixane_Uriben_Nalidixic acid_Wintom...   \n",
       "4     Indocid ret_NSC-757061_Indoderm_Rheumacin la_I...   \n",
       "...                                                 ...   \n",
       "2539  Histamine dihydrocloride_Histamine dihydrochlo...   \n",
       "2540  RO 40-5967/001_Mibefradil hydrochloride_Posico...   \n",
       "2541  Xanthinol nicotinate_Complamin ret_Complamin_X...   \n",
       "2542  Meclofenamate sodium hydrate_Sodium meclofenam...   \n",
       "2543  Apazone dihydrate_Azapropazone_Rheumox 600_Cin...   \n",
       "\n",
       "                            Unnamed: 32  Unnamed: 33  Unnamed: 34 Unnamed: 35  \\\n",
       "0                                   NaN          NaN          NaN         NaN   \n",
       "1                                   NaN          NaN          NaN         NaN   \n",
       "2                                   NaN          NaN          NaN         NaN   \n",
       "3     320_Nalidixate_Mictral_NSC-82174_          NaN          NaN         NaN   \n",
       "4                                   NaN          NaN          NaN         NaN   \n",
       "...                                 ...          ...          ...         ...   \n",
       "2539                                NaN          NaN          NaN         NaN   \n",
       "2540                                NaN          NaN          NaN         NaN   \n",
       "2541                                NaN          NaN          NaN         NaN   \n",
       "2542                                NaN          NaN          NaN         NaN   \n",
       "2543                                NaN          NaN          NaN         NaN   \n",
       "\n",
       "      Unnamed: 36  Unnamed: 37  Unnamed: 38  Unnamed: 39  \n",
       "0             NaN          NaN          NaN          NaN  \n",
       "1             NaN          NaN          NaN          NaN  \n",
       "2             NaN          NaN          NaN          NaN  \n",
       "3             NaN          NaN          NaN          NaN  \n",
       "4             NaN          NaN          NaN          NaN  \n",
       "...           ...          ...          ...          ...  \n",
       "2539          NaN          NaN          NaN          NaN  \n",
       "2540          NaN          NaN          NaN          NaN  \n",
       "2541          NaN          NaN          NaN          NaN  \n",
       "2542          NaN          NaN          NaN          NaN  \n",
       "2543          NaN          NaN          NaN          NaN  \n",
       "\n",
       "[2544 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a89a76b-f705-40ae-86a3-74e984e23c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_paper_drug_df = drug_info_df['smiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "261afbf3-466a-4e4c-a322-45d8e8afcef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_paper_df_1 = pd.read_csv('/Users/alexwang/Documents/GitHub/DrugWithdrawn/split/db_no_agree_no_dups/ChEMBL/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "621940ad-6421-4384-b097-7d7e4624ec20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>smiles</th>\n",
       "      <th>length</th>\n",
       "      <th>inchikey</th>\n",
       "      <th>name</th>\n",
       "      <th>groups</th>\n",
       "      <th>withdrawn_class</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Cc1c(N)nc([C@H](CC(N)=O)NC[C@H](N)C(N)=O)nc1C(...</td>\n",
       "      <td>296</td>\n",
       "      <td>WUIABRMSWOKTOF-OYALTWQYSA-O</td>\n",
       "      <td>bleomycin sulfate</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>['ChEMBL']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>CN[C@H](CC(C)C)C(=O)N[C@H]1C(=O)N[C@@H](CC(N)=...</td>\n",
       "      <td>292</td>\n",
       "      <td>LCTORFDMHNKUSG-XTTLPDOESA-N</td>\n",
       "      <td>vancomycin hydrochloride</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>['ChEMBL']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NC[C@H]1O[C@H](O[C@@H]2[C@@H](N)C[C@@H](N)[C@H...</td>\n",
       "      <td>291</td>\n",
       "      <td>NZKFUBQRAWPZJP-BXKLGIMVSA-N</td>\n",
       "      <td>tobramycin sulfate</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>['ChEMBL']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>CN[C@H](CC(C)C)C(=O)N[C@@H]1[C@H](O)C2=CC=C(OC...</td>\n",
       "      <td>289</td>\n",
       "      <td>MYPYJXKWCTUITO-LYRMYLQWSA-N</td>\n",
       "      <td>vancomycin</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>['DrugBank', 'ChEMBL', 'NCATS']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>CCCCCOc1ccc(-c2cc(-c3ccc(C(=O)N[C@H]4C[C@@H](O...</td>\n",
       "      <td>266</td>\n",
       "      <td>KOOAFHGJVIVFMZ-WZPXRXMFSA-M</td>\n",
       "      <td>micafungin sodium</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>['ChEMBL']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>6513</td>\n",
       "      <td>6513</td>\n",
       "      <td>CC(C)O</td>\n",
       "      <td>6</td>\n",
       "      <td>KFZMGEQAYNKOFK-UHFFFAOYSA-N</td>\n",
       "      <td>isopropyl alcohol</td>\n",
       "      <td>approved; investigational</td>\n",
       "      <td>0</td>\n",
       "      <td>['DrugBank', 'ChEMBL']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>6520</td>\n",
       "      <td>6520</td>\n",
       "      <td>C=CCl</td>\n",
       "      <td>5</td>\n",
       "      <td>BZHJMEDXRYGGRV-UHFFFAOYSA-N</td>\n",
       "      <td>vinyl chloride</td>\n",
       "      <td>withdrawn</td>\n",
       "      <td>1</td>\n",
       "      <td>['ChEMBL', 'NCATS']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>6523</td>\n",
       "      <td>6523</td>\n",
       "      <td>[N]=O</td>\n",
       "      <td>5</td>\n",
       "      <td>MWUXSHHQAYIFBG-UHFFFAOYSA-N</td>\n",
       "      <td>nitric oxide</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>['DrugBank', 'ChEMBL']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>6530</td>\n",
       "      <td>6530</td>\n",
       "      <td>NCCS</td>\n",
       "      <td>4</td>\n",
       "      <td>UFULAYFCSOUIOV-UHFFFAOYSA-N</td>\n",
       "      <td>cysteamine</td>\n",
       "      <td>approved; investigational</td>\n",
       "      <td>0</td>\n",
       "      <td>['DrugBank', 'ChEMBL', 'NCATS']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>6532</td>\n",
       "      <td>6532</td>\n",
       "      <td>ON=O</td>\n",
       "      <td>4</td>\n",
       "      <td>IOVCWXUNBOPUCH-UHFFFAOYSA-N</td>\n",
       "      <td>nitrous acid</td>\n",
       "      <td>approved; investigational</td>\n",
       "      <td>0</td>\n",
       "      <td>['DrugBank', 'ChEMBL', 'NCATS']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2565 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  index                                             smiles  \\\n",
       "0              2      2  Cc1c(N)nc([C@H](CC(N)=O)NC[C@H](N)C(N)=O)nc1C(...   \n",
       "1              6      6  CN[C@H](CC(C)C)C(=O)N[C@H]1C(=O)N[C@@H](CC(N)=...   \n",
       "2              8      8  NC[C@H]1O[C@H](O[C@@H]2[C@@H](N)C[C@@H](N)[C@H...   \n",
       "3             12     12  CN[C@H](CC(C)C)C(=O)N[C@@H]1[C@H](O)C2=CC=C(OC...   \n",
       "4             27     27  CCCCCOc1ccc(-c2cc(-c3ccc(C(=O)N[C@H]4C[C@@H](O...   \n",
       "...          ...    ...                                                ...   \n",
       "2560        6513   6513                                             CC(C)O   \n",
       "2561        6520   6520                                              C=CCl   \n",
       "2562        6523   6523                                              [N]=O   \n",
       "2563        6530   6530                                               NCCS   \n",
       "2564        6532   6532                                               ON=O   \n",
       "\n",
       "      length                     inchikey                      name  \\\n",
       "0        296  WUIABRMSWOKTOF-OYALTWQYSA-O         bleomycin sulfate   \n",
       "1        292  LCTORFDMHNKUSG-XTTLPDOESA-N  vancomycin hydrochloride   \n",
       "2        291  NZKFUBQRAWPZJP-BXKLGIMVSA-N        tobramycin sulfate   \n",
       "3        289  MYPYJXKWCTUITO-LYRMYLQWSA-N                vancomycin   \n",
       "4        266  KOOAFHGJVIVFMZ-WZPXRXMFSA-M         micafungin sodium   \n",
       "...      ...                          ...                       ...   \n",
       "2560       6  KFZMGEQAYNKOFK-UHFFFAOYSA-N         isopropyl alcohol   \n",
       "2561       5  BZHJMEDXRYGGRV-UHFFFAOYSA-N            vinyl chloride   \n",
       "2562       5  MWUXSHHQAYIFBG-UHFFFAOYSA-N              nitric oxide   \n",
       "2563       4  UFULAYFCSOUIOV-UHFFFAOYSA-N                cysteamine   \n",
       "2564       4  IOVCWXUNBOPUCH-UHFFFAOYSA-N              nitrous acid   \n",
       "\n",
       "                         groups  withdrawn_class  \\\n",
       "0                      approved                0   \n",
       "1                      approved                0   \n",
       "2                      approved                0   \n",
       "3                      approved                0   \n",
       "4                      approved                0   \n",
       "...                         ...              ...   \n",
       "2560  approved; investigational                0   \n",
       "2561                  withdrawn                1   \n",
       "2562                   approved                0   \n",
       "2563  approved; investigational                0   \n",
       "2564  approved; investigational                0   \n",
       "\n",
       "                               source  \n",
       "0                          ['ChEMBL']  \n",
       "1                          ['ChEMBL']  \n",
       "2                          ['ChEMBL']  \n",
       "3     ['DrugBank', 'ChEMBL', 'NCATS']  \n",
       "4                          ['ChEMBL']  \n",
       "...                               ...  \n",
       "2560           ['DrugBank', 'ChEMBL']  \n",
       "2561              ['ChEMBL', 'NCATS']  \n",
       "2562           ['DrugBank', 'ChEMBL']  \n",
       "2563  ['DrugBank', 'ChEMBL', 'NCATS']  \n",
       "2564  ['DrugBank', 'ChEMBL', 'NCATS']  \n",
       "\n",
       "[2565 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_paper_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1bf7c442-e0dc-432a-9aac-92f9c1218521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>smiles</th>\n",
       "      <th>length</th>\n",
       "      <th>inchikey</th>\n",
       "      <th>name</th>\n",
       "      <th>groups</th>\n",
       "      <th>withdrawn_class</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, index, smiles, length, inchikey, name, groups, withdrawn_class, source]\n",
       "Index: []"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_paper_df_1[(cs_paper_df_1['groups'] == 'withdrawn') & (cs_paper_df_1['withdrawn_class'] == 1) & (cs_paper_df_1['source'] == 'ChEMBL')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd671e9e-af27-4a07-bb23-dad1f2155479",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_paper_df_2 = pd.read_csv('/Users/alexwang/Documents/GitHub/DrugWithdrawn/split/db_no_agree_no_dups/ChEMBL/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0f8061b-6371-4b00-8444-21d9ec3ffd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>smiles</th>\n",
       "      <th>length</th>\n",
       "      <th>inchikey</th>\n",
       "      <th>name</th>\n",
       "      <th>groups</th>\n",
       "      <th>withdrawn_class</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CC(C)C[C@@]([H])(C(=N[C@@]([H])(CCCCNC(C)C)C(=...</td>\n",
       "      <td>300</td>\n",
       "      <td>MEUCPCLKGZSHTA-YAVPXVOBSA-N</td>\n",
       "      <td>degarelix acetate</td>\n",
       "      <td>US Approved Rx</td>\n",
       "      <td>0</td>\n",
       "      <td>['NCATS']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>CC(C)C[C@@]([H])(C(=N[C@@]([H])(CCCCNC(C)C)C(=...</td>\n",
       "      <td>299</td>\n",
       "      <td>AUTFSFUMNFDPLH-KYMMNHPFSA-N</td>\n",
       "      <td>degarelix acetate anhydrous</td>\n",
       "      <td>US Approved Rx</td>\n",
       "      <td>0</td>\n",
       "      <td>['NCATS']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Cc1c(C(=N[C@@]([H])([C@]([H])(c2cnc[nH]2)O[C@@...</td>\n",
       "      <td>294</td>\n",
       "      <td>BODDZCXQPQPRES-OYALTWQYSA-N</td>\n",
       "      <td>bleomycin a2 hydrochloride</td>\n",
       "      <td>US Approved Rx</td>\n",
       "      <td>0</td>\n",
       "      <td>['NCATS']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>CO[C@H]1O[C@H](COS(O)(=O)=O)[C@@H](O[C@@H]2O[C...</td>\n",
       "      <td>293</td>\n",
       "      <td>KANJSNBRCNMZMV-ABRZTLGGSA-N</td>\n",
       "      <td>fondaparinux</td>\n",
       "      <td>approved; investigational</td>\n",
       "      <td>0</td>\n",
       "      <td>['DrugBank', 'NCATS']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>C[C@@H](O)[C@@H]1NC(=O)[C@H](CC2=CC=CC=C2)NC(=...</td>\n",
       "      <td>292</td>\n",
       "      <td>NHXLMOGPVYXJNR-ATOGVRKGSA-N</td>\n",
       "      <td>somatostatin</td>\n",
       "      <td>approved; investigational</td>\n",
       "      <td>0</td>\n",
       "      <td>['DrugBank']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td>6526</td>\n",
       "      <td>6526</td>\n",
       "      <td>C(#N)S</td>\n",
       "      <td>4</td>\n",
       "      <td>ZMZDMBWJUHKJPS-UHFFFAOYSA-N</td>\n",
       "      <td>thiocyanic acid</td>\n",
       "      <td>US Previously Marketed</td>\n",
       "      <td>1</td>\n",
       "      <td>['NCATS']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3775</th>\n",
       "      <td>6527</td>\n",
       "      <td>6527</td>\n",
       "      <td>ClCl</td>\n",
       "      <td>4</td>\n",
       "      <td>KZBUYRJDOAKODT-UHFFFAOYSA-N</td>\n",
       "      <td>chlorine</td>\n",
       "      <td>US Previously Marketed</td>\n",
       "      <td>1</td>\n",
       "      <td>['NCATS']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>6528</td>\n",
       "      <td>6528</td>\n",
       "      <td>CCCl</td>\n",
       "      <td>4</td>\n",
       "      <td>HRYZWHHZPQKTII-UHFFFAOYSA-N</td>\n",
       "      <td>ethyl chloride</td>\n",
       "      <td>US Previously Marketed</td>\n",
       "      <td>1</td>\n",
       "      <td>['NCATS']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>6531</td>\n",
       "      <td>6531</td>\n",
       "      <td>CCCO</td>\n",
       "      <td>4</td>\n",
       "      <td>BDERNNFJNOPAEC-UHFFFAOYSA-N</td>\n",
       "      <td>propyl alcohol</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>['DrugBank']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>6533</td>\n",
       "      <td>6533</td>\n",
       "      <td>NCCN</td>\n",
       "      <td>4</td>\n",
       "      <td>PIICEJLVQHRZGT-UHFFFAOYSA-N</td>\n",
       "      <td>ethylenediamine</td>\n",
       "      <td>approved; experimental</td>\n",
       "      <td>0</td>\n",
       "      <td>['DrugBank']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3779 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  index                                             smiles  \\\n",
       "0              0      0  CC(C)C[C@@]([H])(C(=N[C@@]([H])(CCCCNC(C)C)C(=...   \n",
       "1              1      1  CC(C)C[C@@]([H])(C(=N[C@@]([H])(CCCCNC(C)C)C(=...   \n",
       "2              3      3  Cc1c(C(=N[C@@]([H])([C@]([H])(c2cnc[nH]2)O[C@@...   \n",
       "3              4      4  CO[C@H]1O[C@H](COS(O)(=O)=O)[C@@H](O[C@@H]2O[C...   \n",
       "4              5      5  C[C@@H](O)[C@@H]1NC(=O)[C@H](CC2=CC=CC=C2)NC(=...   \n",
       "...          ...    ...                                                ...   \n",
       "3774        6526   6526                                             C(#N)S   \n",
       "3775        6527   6527                                               ClCl   \n",
       "3776        6528   6528                                               CCCl   \n",
       "3777        6531   6531                                               CCCO   \n",
       "3778        6533   6533                                               NCCN   \n",
       "\n",
       "      length                     inchikey                         name  \\\n",
       "0        300  MEUCPCLKGZSHTA-YAVPXVOBSA-N            degarelix acetate   \n",
       "1        299  AUTFSFUMNFDPLH-KYMMNHPFSA-N  degarelix acetate anhydrous   \n",
       "2        294  BODDZCXQPQPRES-OYALTWQYSA-N   bleomycin a2 hydrochloride   \n",
       "3        293  KANJSNBRCNMZMV-ABRZTLGGSA-N                 fondaparinux   \n",
       "4        292  NHXLMOGPVYXJNR-ATOGVRKGSA-N                 somatostatin   \n",
       "...      ...                          ...                          ...   \n",
       "3774       4  ZMZDMBWJUHKJPS-UHFFFAOYSA-N              thiocyanic acid   \n",
       "3775       4  KZBUYRJDOAKODT-UHFFFAOYSA-N                     chlorine   \n",
       "3776       4  HRYZWHHZPQKTII-UHFFFAOYSA-N               ethyl chloride   \n",
       "3777       4  BDERNNFJNOPAEC-UHFFFAOYSA-N               propyl alcohol   \n",
       "3778       4  PIICEJLVQHRZGT-UHFFFAOYSA-N              ethylenediamine   \n",
       "\n",
       "                         groups  withdrawn_class                 source  \n",
       "0                US Approved Rx                0              ['NCATS']  \n",
       "1                US Approved Rx                0              ['NCATS']  \n",
       "2                US Approved Rx                0              ['NCATS']  \n",
       "3     approved; investigational                0  ['DrugBank', 'NCATS']  \n",
       "4     approved; investigational                0           ['DrugBank']  \n",
       "...                         ...              ...                    ...  \n",
       "3774     US Previously Marketed                1              ['NCATS']  \n",
       "3775     US Previously Marketed                1              ['NCATS']  \n",
       "3776     US Previously Marketed                1              ['NCATS']  \n",
       "3777                   approved                0           ['DrugBank']  \n",
       "3778     approved; experimental                0           ['DrugBank']  \n",
       "\n",
       "[3779 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_paper_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "181824d7-83cd-4e52-9f29-832aefd568a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_paper_df_1 = cs_paper_df_1['smiles'].drop_duplicates().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8e5ee57-0bb8-4ead-a098-9df555945a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_paper_df_2 = cs_paper_df_2['smiles'].drop_duplicates().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bcf5e7f-1864-4e1e-a633-658c589eba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_paper_df = pd.concat([cs_paper_df_1, cs_paper_df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cfa335b-bbac-4535-a92d-98a392ccbcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cc1c(N)nc([C@H](CC(N)=O)NC[C@H](N)C(N)=O)nc1C(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN[C@H](CC(C)C)C(=O)N[C@H]1C(=O)N[C@@H](CC(N)=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NC[C@H]1O[C@H](O[C@@H]2[C@@H](N)C[C@@H](N)[C@H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CN[C@H](CC(C)C)C(=O)N[C@@H]1[C@H](O)C2=CC=C(OC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCCCCOc1ccc(-c2cc(-c3ccc(C(=O)N[C@H]4C[C@@H](O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td>C(#N)S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3775</th>\n",
       "      <td>ClCl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>CCCl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>CCCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>NCCN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6334 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 smiles\n",
       "0     Cc1c(N)nc([C@H](CC(N)=O)NC[C@H](N)C(N)=O)nc1C(...\n",
       "1     CN[C@H](CC(C)C)C(=O)N[C@H]1C(=O)N[C@@H](CC(N)=...\n",
       "2     NC[C@H]1O[C@H](O[C@@H]2[C@@H](N)C[C@@H](N)[C@H...\n",
       "3     CN[C@H](CC(C)C)C(=O)N[C@@H]1[C@H](O)C2=CC=C(OC...\n",
       "4     CCCCCOc1ccc(-c2cc(-c3ccc(C(=O)N[C@H]4C[C@@H](O...\n",
       "...                                                 ...\n",
       "3774                                             C(#N)S\n",
       "3775                                               ClCl\n",
       "3776                                               CCCl\n",
       "3777                                               CCCO\n",
       "3778                                               NCCN\n",
       "\n",
       "[6334 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_paper_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed3551-0d32-408d-9950-1d9b833491f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
